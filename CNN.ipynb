{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "import keras.layers\n",
    "from keras import backend as K\n",
    "from keras import regularizers\n",
    "from keras.utils import to_categorical\n",
    "import sys\n",
    "from collections import OrderedDict\n",
    "import os\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "sys.path.append(os.path.join(os.path.abspath(\"..\"),'lib'))\n",
    "sys.path.append(\"/home/pablo/Documents/Master/Icecube/DeepIceLearning-master/lib\")\n",
    "import transformations as tr\n",
    "import numpy as np\n",
    "import keras.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add path to own libs\n",
    "import sys\n",
    "sys.path.append(\"../lib/\")\n",
    "sys.path.append(\"/home/pablo/Documents/Master/Icecube/DeepIceLearning-master/lib\")\n",
    "sys.path.append(\"/home/pablo/Documents/Master/Icecube/DeepIceLearning-master/lib\")\n",
    "\n",
    "\n",
    "# Keras Imports\n",
    "import keras \n",
    "import keras.layers\n",
    "from keras.callbacks import CSVLogger, EarlyStopping\n",
    "import keras.backend as K\n",
    "from keras.utils import plot_model\n",
    "\n",
    "# Own Imports\n",
    "#import block_units as bunit\n",
    "#from functions import generator, IC_identity, IC_divide_1000, IC_log10, get_indices\n",
    "import transformations as tr\n",
    "#from plotting_style import figsize\n",
    "#from custom import generators\n",
    "from functions import generator_v2\n",
    "\n",
    "#Various Imports\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(batch_size, file_handlers, inds,\n",
    "              inp_transformations, out_transformations,\n",
    "              weighting_function=None, use_data=False, equal_len=False,\n",
    "              mask_func=None):\n",
    "\n",
    "    \"\"\" This function generates the training batches for the neural network.\n",
    "    It load all input and output data and applies the transformations\n",
    "    as defined in the network definition file.\n",
    "\n",
    "    Arguments:\n",
    "    batch size : the batch size per gpu\n",
    "    file_handlers: list of files used for the training\n",
    "                   i.e. ['/path/to/file/A', 'path/to/file/B']\n",
    "    inds: the index range used for the files\n",
    "          e.g. [(0,1000), (0,2000)]\n",
    "    inp_shape_dict: A dictionary with the input shape for each branch\n",
    "    inp_transformations: Dictionary with input variable name and function\n",
    "    out_shape_dict: A dictionary with the output shape for each branch\n",
    "    out_transformations: Dictionary with out variable name and function\n",
    "    weighting_function: A function that returns the event weights on basis\n",
    "                        of the information saved in reco_vals, e.g.\n",
    "                        lambda mc: np.log10(mc['trunc_e'])\n",
    "    mask_func: a function that returns a mask of values that get a \n",
    "                weight of zero, i.e. will not be considered in the loss\n",
    "                e.g. lambda mc: mc['mu_e_on_entry'] < 1.e2\n",
    "                        \n",
    "    Returns:\n",
    "    batch_input : a batch of input data\n",
    "    batch_out: a batch of output data\n",
    "    weights: a weight for each event\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "#     print('Run with inds {}'.format(inds))\n",
    "\n",
    "    in_branches = [(branch, inp_transformations[branch]['general'])\n",
    "                   for branch in inp_transformations]\n",
    "    out_branches = [(branch, out_transformations[branch]['general'])\n",
    "                    for branch in out_transformations]\n",
    "    inp_variables = [[(i, inp_transformations[branch[0]][i][1])\n",
    "                      for i in inp_transformations[branch[0]] if i != 'general']\n",
    "                     for branch in in_branches]\n",
    "    out_variables = [[(i, out_transformations[branch[0]][i][1])\n",
    "                      for i in out_transformations[branch[0]] if i != 'general']\n",
    "                     for branch in out_branches]\n",
    "    cur_file = 0\n",
    "    ind_lo = inds[0][0]\n",
    "    ind_hi = inds[0][0] + batch_size\n",
    "    in_data = h5py.File(file_handlers[0], 'r')\n",
    "    f_reco_vals = in_data['reco_vals']\n",
    "    t0 = time.time()\n",
    "    num_batches = 0\n",
    " \n",
    "    while True:\n",
    "        inp_data = []\n",
    "        out_data = []\n",
    "        weights = []\n",
    "        arr_size = np.min([batch_size, ind_hi - ind_lo])\n",
    "        reco_vals = f_reco_vals[ind_lo:ind_hi]\n",
    "\n",
    "        #print('Generate Input Data')\n",
    "        for k, b in enumerate(out_branches):\n",
    "            for j, f in enumerate(out_variables[k]):\n",
    "                if weighting_function != None:\n",
    "                    tweights=weighting_function(reco_vals)\n",
    "                else:\n",
    "                    tweights=np.ones(arr_size)\n",
    "                if mask_func != None:\n",
    "                    mask = mask_func(reco_vals)\n",
    "                    tweights[mask] = 0\n",
    "            weights.append(tweights)\n",
    "            \n",
    "        for k, b in enumerate(in_branches):\n",
    "            batch_input = np.zeros((arr_size,)+in_branches[k][1])\n",
    "            for j, f in enumerate(inp_variables[k]):\n",
    "                if f[0] in in_data.keys():\n",
    "                    pre_data = np.array(np.squeeze(in_data[f[0]][ind_lo:ind_hi]), ndmin=4)\n",
    "                    batch_input[:,:,:,:,j] = np.atleast_1d(f[1](pre_data))\n",
    "                else:\n",
    "                    pre_data = np.squeeze(reco_vals[f[0]])\n",
    "                    batch_input[:,j]=f[1](pre_data)\n",
    "            inp_data.append(batch_input)\n",
    "            \n",
    "        # Generate Output Data\n",
    "        for k, b in enumerate(out_branches):\n",
    "            if use_data:\n",
    "                continue\n",
    "            shape = (arr_size,)+out_branches[k][1]\n",
    "            batch_output = np.zeros(shape)\n",
    "            for j, f in enumerate(out_variables[k]):\n",
    "                pre_data = np.squeeze(reco_vals[f[0]])\n",
    "                if len(out_variables[k]) == 1:\n",
    "                    batch_output[:]=np.reshape(f[1](pre_data), shape)\n",
    "                else:\n",
    "                    batch_output[:,j] = f[1](pre_data)\n",
    "            out_data.append(batch_output)\n",
    "\n",
    "        #Prepare Next Loop\n",
    "        ind_lo += batch_size\n",
    "        ind_hi += batch_size\n",
    "        if (ind_lo >= inds[cur_file][1]) | (equal_len & (ind_hi > inds[cur_file][1])):\n",
    "            cur_file += 1\n",
    "            if cur_file == len(file_handlers):\n",
    "                cur_file=0\n",
    "            t1 = time.time()\n",
    "#             print('\\n Open File: {} \\n'.format(file_handlers[cur_file]))\n",
    "#             print('\\n Average Time per Batch: {}s \\n'.format((t1-t0)/num_batches))\n",
    "            t0 = time.time()\n",
    "            num_batches = 0\n",
    "            in_data.close()\n",
    "            in_data = h5py.File(file_handlers[cur_file], 'r')\n",
    "            f_reco_vals = in_data['reco_vals']\n",
    "            ind_lo = inds[cur_file][0]\n",
    "            ind_hi = ind_lo + batch_size\n",
    "        elif ind_hi > inds[cur_file][1]:\n",
    "            ind_hi = inds[cur_file][1]\n",
    "       \n",
    "        # Yield Result\n",
    "        num_batches += 1\n",
    "        if use_data:\n",
    "            yield inp_data\n",
    "        else:\n",
    "            yield (inp_data, out_data, weights)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### You don't necessarily have to change anything here\n",
    "\n",
    "# Files for training\n",
    "\n",
    "dnn_files={'files_training' : ['h5_final/File_19.h5',\n",
    "                               'h5_final/File_10.h5',\n",
    "                               'h5_final/File_11.h5',],\n",
    "           'files_validation' : ['h5_final/File_14.h5'],\n",
    "           'files_test' : ['h5_final/File_15.h5'], }\n",
    "           \n",
    "# Size of the mini-batches\n",
    "batch_size = 68\n",
    "\n",
    "# For imbalanced data an event weighting can be useful\n",
    "# if you want to use an event weight define a function that uses the reco vals\n",
    "# to calculate a weight, e.g.: lambda mc: np.log10(mc['trunc_e'])\n",
    "sample_weights = None #lambda mc: np.log10(mc['trunc_e'])**(1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input\n",
    "\n",
    "### Add here the features you want to use #####\n",
    "\n",
    "# Each input branch is a dictionary containing information about the input features.\n",
    "# Single features should have the syntax: 'feature': (shape, transformation), where shape is\n",
    "# in our case always (10,10,60) so just copy and paste. Also add a key 'general' that gives the\n",
    "# overall shape of the branch, i.e. (10,10,60, number of features)\n",
    "\n",
    "\n",
    "grid_shape = (10, 10, 60)\n",
    "input_branch1 = {'IC_charge': (grid_shape,  tr.IC_divide_100) ,\n",
    "                 'IC_mean': (grid_shape, tr.IC_divide_100),\n",
    "                 'IC_mult': (grid_shape, tr.IC_divide_1000),\n",
    "                 'IC_num_pulses': (grid_shape, tr.IC_divide_1000),\n",
    "                 'general': grid_shape+ (4,)}\n",
    "\n",
    "\n",
    "inp_shapes = OrderedDict([('Branch_IC_time', input_branch1)])\n",
    "\n",
    "# Output\n",
    "\n",
    "### Define here your output variable(s) ####\n",
    "\n",
    "# Syntax is the same as for the input case, but make sure that shape is always a tuple,\n",
    "# so for one float use (1,), for two (2,) and so on\n",
    "output_branch1 = {'ClassificationLabel': ((2, ), tr.oneHotEncode_01), # Predefined Classification Label, transformed to the classes that should get predicted\n",
    "                  'general': (2,)}\n",
    "out_shapes = OrderedDict([('Out1', output_branch1)]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ClassificationLabel': ((2,),\n",
       "  <function transformations.oneHotEncode_01(x, r_vals=None)>),\n",
       " 'general': (2,)}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_branch1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Most important: Define your model using the functional API of Keras\n",
    "# https://keras.io/getting-started/functional-api-guide/\n",
    "\n",
    "# The Input\n",
    "input_b1 = keras.layers.Input(\n",
    "    shape=inp_shapes['Branch_IC_time']['general'],\n",
    "    name = \"Input-Branch1\")\n",
    "\n",
    "# Convolutional Layers\n",
    "z1 = keras.layers.Conv3D(36, (1, 1, 1), activation='relu', padding=\"same\", name='conv1x1x1')(input_b1)\n",
    "#z1 = keras.layers.BatchNormalization()(z1)\n",
    "#z1 = keras.layers.Conv3D(16, (3, 3, 4), activation='relu',padding=\"same\", name='conv3x3x4')(input_b1)\n",
    "#z1 = keras.layers.BatchNormalization()(z1)\n",
    "#z1 = keras.layers.MaxPooling3D(pool_size=(2, 2, 3))(z1)\n",
    "\n",
    "#z1 = keras.layers.Conv3D(16, (3, 3, 3), activation='relu',padding=\"same\", name='conv3x3x3_1')(input_b1)\n",
    "#z1 = keras.layers.BatchNormalization()(z1)\n",
    "#z1 = keras.layers.Conv3D(16, (3, 3, 3), activation='relu', padding=\"same\", name='conv3x3x3_2')(input_b1)\n",
    "#z1 = keras.layers.MaxPooling3D(pool_size=(1, 1, 2))(z1)\n",
    "#z1 = keras.layers.Conv3D(8, (2, 2, 2), activation='relu',padding=\"same\", name='conv2x2x2')(input_b1)\n",
    "#z1 = keras.layers.BatchNormalization()(z1)\n",
    "#z1 = keras.layers.Flatten()(z1)\n",
    "\n",
    "# Fully Connected Layers\n",
    "#z1 = keras.layers.Dense(32, activation='relu',name='Dense1')(z1)\n",
    "#z1 = keras.layers.BatchNormalization()(z1)\n",
    "#z1 = keras.layers.Dropout(0.3)(z1)\n",
    "#z1 = keras.layers.Dense(64, activation='relu', name='Dense2')(z1)\n",
    "#z1 = keras.layers.BatchNormalization()(z1)\n",
    "#z1 = keras.layers.Dropout(0.3)(z1)\n",
    "\n",
    "# The Output\n",
    "output_b1 = keras.layers.Dense(4, activation='softmax', name='Target1')(z1)\n",
    "model= keras.models.Model(inputs=[input_b1], outputs=[output_b1])\n",
    "\n",
    "# Print a summary of the model\n",
    "# print(model.summary())\n",
    "\n",
    "#choose an optimizer an compile the model\n",
    "# plot_model(model, to_file='./model.png')\n",
    "model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_10\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input-Branch1 (InputLayer)   (None, 10, 10, 60, 4)     0         \n",
      "_________________________________________________________________\n",
      "conv1x1x1 (Conv3D)           (None, 10, 10, 60, 36)    180       \n",
      "_________________________________________________________________\n",
      "Target1 (Dense)              (None, 10, 10, 60, 4)     148       \n",
      "=================================================================\n",
      "Total params: 328\n",
      "Trainable params: 328\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/pablo/anaconda3/envs/env37/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pablo/anaconda3/envs/env37/lib/python3.7/site-packages/keras/utils/data_utils.py:718: UserWarning: An input could not be retrieved. It could be because a worker has died.We do not have any information on the lost sample.\n",
      "  UserWarning)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-71-2eb19eb00afd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m     use_multiprocessing=True)\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/env37/lib/python3.7/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/env37/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1730\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1731\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1732\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1733\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/env37/lib/python3.7/site-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    183\u001b[0m             \u001b[0mbatch_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0msteps_done\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m                 \u001b[0mgenerator_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__len__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/env37/lib/python3.7/site-packages/keras/utils/data_utils.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    709\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m                     \u001b[0mfuture\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 711\u001b[0;31m                     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    712\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtask_done\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    713\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTimeoutError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/env37/lib/python3.7/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    649\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 651\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    652\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    653\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/env37/lib/python3.7/multiprocessing/pool.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    646\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    647\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 648\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    649\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/env37/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 552\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    553\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/env37/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    301\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Fit the Model\n",
    "# The generator prepares mini-batches that are fed into the network\n",
    "# For more options and callbacks see https://keras.io/models/sequential/\n",
    "# as well as the docstrings of the generator in functions.py\n",
    "\n",
    "### There are a few things that could be changed here, e.g. the 'early stopping'\n",
    "\n",
    "\n",
    "train_steps, valid_steps, test_steps, train_inds, valid_inds, test_inds = get_indices(dnn_files, batch_size) \n",
    "\n",
    "model.fit_generator(\n",
    "    generator(\n",
    "        batch_size,\n",
    "        dnn_files['files_training'],\n",
    "        train_inds,\n",
    "        inp_shapes, out_shapes,\n",
    "        weighting_function=sample_weights,\n",
    "        mask_func=None\n",
    "    ),\n",
    "    steps_per_epoch=train_steps,\n",
    "    validation_data=generator(\n",
    "        batch_size,\n",
    "        dnn_files['files_validation'],\n",
    "        valid_inds,\n",
    "        inp_shapes, out_shapes,\n",
    "        weighting_function=sample_weights,\n",
    "        mask_func=None\n",
    "    ),\n",
    "    validation_steps=valid_steps,\n",
    "    epochs=10, # Maximum number of epoch for the training\n",
    "    callbacks = [\n",
    "        EarlyStopping(patience=1) # Stop once the validation loss did not decrease for one epoch\n",
    "    ], \n",
    "    verbose=1,\n",
    "    max_queue_size=2,\n",
    "    use_multiprocessing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pablo/anaconda3/envs/env37/lib/python3.7/site-packages/ipykernel_launcher.py:1: H5pyDeprecationWarning: The default file mode will change to 'r' (read-only) in h5py 3.0. To suppress this warning, pass the mode you need to h5py.File(), or set the global default h5.get_config().default_file_mode, or set the environment variable H5PY_DEFAULT_READONLY=1. Available modes are: 'r', 'r+', 'w', 'w-'/'x', 'a'. See the docs for details.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "data = h5py.File('h5_final/File_19.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pablo/anaconda3/envs/env37/lib/python3.7/site-packages/ipykernel_launcher.py:1: H5pyDeprecationWarning: The default file mode will change to 'r' (read-only) in h5py 3.0. To suppress this warning, pass the mode you need to h5py.File(), or set the global default h5.get_config().default_file_mode, or set the environment variable H5PY_DEFAULT_READONLY=1. Available modes are: 'r', 'r+', 'w', 'w-'/'x', 'a'. See the docs for details.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "x = h5py.File('h5_final/File_19.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
